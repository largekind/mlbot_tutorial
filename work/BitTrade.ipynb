{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c98a6a-835e-4938-897e-b351053366d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      timestamp        open        high         low       close        volume\n",
      "0    2017-10-31   679910.34   727288.22   674553.25   723579.09  4.728300e+10\n",
      "1    2017-11-01   723751.37   766552.45   713575.88   764334.90  5.844858e+10\n",
      "2    2017-11-02   764247.87   857528.51   761617.11   805165.37  9.242606e+10\n",
      "3    2017-11-03   805246.62   853429.62   786480.90   823854.18  8.196366e+10\n",
      "4    2017-11-04   823851.31   845389.80   800200.82   839556.37  6.597304e+10\n",
      "...         ...         ...         ...         ...         ...           ...\n",
      "1996 2023-04-19  4069137.27  4076795.31  3872362.29  3893088.33  2.048354e+10\n",
      "1997 2023-04-20  3893088.33  3918645.83  3765607.66  3786412.95  1.844144e+10\n",
      "1998 2023-04-21  3786412.95  3802188.72  3644461.28  3654750.87  1.702854e+10\n",
      "1999 2023-04-22  3654750.87  3754880.86  3641006.51  3747411.86  8.716559e+09\n",
      "2000 2023-04-23  3747411.86  3747981.50  3700333.12  3709590.27  3.535152e+09\n",
      "\n",
      "[2001 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def fetch_cryptocompare_ohlcv_data(fsym, tsym, limit=2000, aggregate=1, interval='day'):\n",
    "    \"\"\"\n",
    "    Cryptocompare APIを使用して、過去のOHLCVデータを取得する関数\n",
    "    \"\"\"\n",
    "    base_url = 'https://min-api.cryptocompare.com/data/v2/histo'\n",
    "    intervals = {'minute': 'minute', 'hour': 'hour', 'day': 'day'}\n",
    "    \n",
    "    if interval not in intervals:\n",
    "        raise ValueError(f\"Invalid interval: {interval}\")\n",
    "\n",
    "    url = f\"{base_url}{intervals[interval]}\"\n",
    "    params = {\n",
    "        'fsym': fsym.upper(),\n",
    "        'tsym': tsym.upper(),\n",
    "        'limit': limit,\n",
    "        'aggregate': aggregate\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()['Data']['Data']\n",
    "    \n",
    "    # データをpandas DataFrameに変換\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volumeto']]\n",
    "    df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_from_csv(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "filename = 'ohlcv_data.csv'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    # ローカルにファイルが存在する場合、データをローカルから読み込む\n",
    "    ohlcv_data = load_from_csv(filename)\n",
    "else:\n",
    "    # ローカルにファイルが存在しない場合、データを取得し、ローカルに保存する\n",
    "    fsym = 'BTC'\n",
    "    tsym = 'JPY'\n",
    "    ohlcv_data = fetch_cryptocompare_ohlcv_data(fsym, tsym)\n",
    "    save_to_csv(ohlcv_data, filename)\n",
    "\n",
    "print(ohlcv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189e68e2-2e81-44ac-ab80-7c1e60c28d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'api_key.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_100/2593722607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# NewsAPIキーを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mapi_key_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'api_key.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# ニュースデータを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_100/2593722607.py\u001b[0m in \u001b[0;36mread_api_key\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'api_key.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def read_api_key(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def fetch_cryptocurrency_news(api_key):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    \n",
    "    # Bitcoinに関連するニュースを検索\n",
    "    parameters = {\n",
    "        'q': 'bitcoin',\n",
    "        'language': 'en',\n",
    "        'sortBy': 'publishedAt',\n",
    "        'apiKey': api_key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=parameters)\n",
    "    data = response.json()\n",
    "    \n",
    "    # 必要なデータを抽出\n",
    "    news_data = []\n",
    "    for article in data['articles']:\n",
    "        news = {\n",
    "            'title': article['title'],\n",
    "            'date': article['publishedAt'],\n",
    "            'url': article['url']\n",
    "        }\n",
    "        news_data.append(news)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "def vectorize_news_data(news_data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    titles = [news['title'] for news in news_data]\n",
    "    vectors = vectorizer.fit_transform(titles)\n",
    "    return vectors\n",
    "\n",
    "# NewsAPIキーを取得\n",
    "api_key_file_path = 'api_key.txt'\n",
    "API_KEY = read_api_key(api_key_file_path)\n",
    "\n",
    "# ニュースデータを取得\n",
    "news_data = fetch_cryptocurrency_news(API_KEY)\n",
    "\n",
    "# ニュースデータをベクトル化\n",
    "news_vectors = vectorize_news_data(news_data)\n",
    "\n",
    "print(news_data)\n",
    "print(news_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1528b-3230-4f44-9971-3eb200b5f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_crypto_and_news_data(crypto_data, news_data, news_vectors):\n",
    "    # ニュースデータを時系列でソート\n",
    "    news_data_sorted = sorted(news_data, key=lambda x: x['date'])\n",
    "\n",
    "    # 日付をインデックスと一致させる\n",
    "    news_data_indexed = {}\n",
    "    for news, vector in zip(news_data_sorted, news_vectors):\n",
    "        date = pd.to_datetime(news['date']).strftime('%Y-%m-%d')\n",
    "        if date not in news_data_indexed:\n",
    "            news_data_indexed[date] = []\n",
    "        news_data_indexed[date].append(vector)\n",
    "\n",
    "    # ニュースのベクトル情報を平均化して仮想通貨のDataFrameに追加\n",
    "    crypto_data['news_vector'] = [np.zeros(news_vectors.shape[1])] * len(crypto_data)\n",
    "    for date, vectors in news_data_indexed.items():\n",
    "        if date in crypto_data.index:\n",
    "            mean_vector = np.mean(vectors, axis=0)\n",
    "            crypto_data.at[date, 'news_vector'] = mean_vector\n",
    "\n",
    "    return crypto_data\n",
    "\n",
    "merged_data = merge_crypto_and_news_data(ohlcv_data, news_data, news_vectors)\n",
    "print(merged_data)\n",
    "merged_data.to_csv(\"merged_crypto_news_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252ba76-b7f6-41d0-b048-3b7ad8f0a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読み込み\n",
    "merged_data = pd.read_csv(\"merged_crypto_news_data.csv\")\n",
    "\n",
    "# 入力データ（ニュース情報のベクトル）とターゲットデータ（価格情報）に分割\n",
    "X = merged_data.drop(columns=['close'])\n",
    "y = merged_data['close']\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d31048-4601-47dc-849d-7d1ad5ec10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class CryptoNewsDataset(Dataset):\n",
    "    def __init__(self, news_vectors, targets):\n",
    "        self.news_vectors = news_vectors\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.news_vectors[idx], self.targets[idx]\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"merged_crypto_news_data.csv\")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize news titles\n",
    "train_encodings = tokenizer(train_data[\"title\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_data[\"title\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create torch datasets\n",
    "train_dataset = CryptoNewsDataset(train_encodings[\"input_ids\"], torch.tensor(train_data[\"close\"].values))\n",
    "val_dataset = CryptoNewsDataset(val_encodings[\"input_ids\"], torch.tensor(val_data[\"close\"].values))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train model\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.MSELoss()(outputs.logits.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_preds.extend(outputs.logits.squeeze().tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "    mse = mean_squared_error(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d392b4-5eef-4f9f-bfa4-d49e702dd6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

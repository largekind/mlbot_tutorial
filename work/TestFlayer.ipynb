{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2cc25f-6603-4846-8e2f-8269cd34ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1時間足データ:\n",
      "            timestamp       open       high        low      close     volume\n",
      "0 2023-04-23 05:00:00  3712326.0  3712375.0  3707648.0  3711225.0  25.599508\n",
      "1 2023-04-23 06:00:00  3711225.0  3711830.0  3709708.0  3709795.0   0.861117\n",
      "15分足データ:\n",
      "            timestamp       open       high        low      close     volume\n",
      "0 2023-04-23 05:30:00  3713155.0  3713155.0  3707965.0  3709721.0  24.521348\n",
      "1 2023-04-23 05:45:00  3709721.0  3711225.0  3707648.0  3711225.0   4.674163\n",
      "2 2023-04-23 06:00:00  3711225.0  3711830.0  3709924.0  3711006.0   0.735498\n"
     ]
    }
   ],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "def fetch_ohlcv(exchange, symbol, timeframe, since, limit):\n",
    "    ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since, limit)\n",
    "    return ohlcv\n",
    "\n",
    "def to_dataframe(ohlcv_data):\n",
    "    df = pd.DataFrame(ohlcv_data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    exchange = ccxt.bitflyer()\n",
    "    symbol = \"BTC/JPY\"\n",
    "    \n",
    "    # 1時間足データの取得\n",
    "    timeframe_hour = \"1h\"\n",
    "    limit_hour = 30000  # 1時間足のデータを1000件取得\n",
    "    since_hour = exchange.parse8601(\"2020-01-01T00:00:00Z\")  # 2020年1月1日からのデータを取得\n",
    "    ohlcv_data_hour = fetch_ohlcv(exchange, symbol, timeframe_hour, since_hour, limit_hour)\n",
    "    df_hour = to_dataframe(ohlcv_data_hour)\n",
    "    print(\"1時間足データ:\")\n",
    "    print(df_hour)\n",
    "    \n",
    "    # 15分足データの取得\n",
    "    timeframe_15min = \"15m\"\n",
    "    limit_15min = 30000  # 15分足のデータを1000件取得\n",
    "    since_15min = exchange.parse8601(\"2020-01-01T00:00:00Z\")  # 2020年1月1日からのデータを取得\n",
    "    ohlcv_data_15min = fetch_ohlcv(exchange, symbol, timeframe_15min, since_15min, limit_15min)\n",
    "    df_15min = to_dataframe(ohlcv_data_15min)\n",
    "    print(\"15分足データ:\")\n",
    "    print(df_15min)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c8a082-3476-4064-86ab-137f46e1b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "X_train length: 0\n",
      "y_train length: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_134/3227451552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_134/3227451552.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.transformer.encoder(x)\n",
    "        x = self.output_fc(x[:, -1])\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "def prepare_data(df, input_cols, output_cols, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    print(len(df))\n",
    "    for i in range(len(df) - window_size):\n",
    "        print(f\"Processing row {i}\")  # デバッグステートメントを追加\n",
    "        X.append(df[input_cols].iloc[i:i + window_size].values)\n",
    "        y.append(df[output_cols].iloc[i + window_size].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    split_index = int(X.shape[0] * 0.8)\n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    print(\"X_train length:\", len(X_train))\n",
    "    print(\"y_train length:\", len(y_train))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def main():\n",
    "    # Get historical data\n",
    "    exchange = ccxt.bitflyer()\n",
    "    symbol = \"BTC/JPY\"\n",
    "\n",
    "    df_hour = pd.DataFrame(exchange.fetch_ohlcv(symbol, \"1h\", limit=1000), columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    df_hour[\"timestamp\"] = pd.to_datetime(df_hour[\"timestamp\"], unit=\"ms\")\n",
    "    \n",
    "    df_15min = pd.DataFrame(exchange.fetch_ohlcv(symbol, \"15m\", limit=1000), columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "    df_15min[\"timestamp\"] = pd.to_datetime(df_15min[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "    # Prepare data\n",
    "    input_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    output_cols = [\"buy_price\", \"sell_price\", \"return\"]\n",
    "    window_size = 50\n",
    "\n",
    "    df_hour[\"buy_price\"] = df_hour[\"close\"] + np.random.uniform(-100, 100, len(df_hour))\n",
    "    df_hour[\"sell_price\"] = df_hour[\"close\"] + np.random.uniform(-100, 100, len(df_hour))\n",
    "    df_hour[\"return\"] = (df_hour[\"sell_price\"] - df_hour[\"buy_price\"]) / df_hour[\"buy_price\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df_hour, input_cols, output_cols, window_size)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Create model\n",
    "    input_dim = len(input_cols)\n",
    "    output_dim = len(output_cols)\n",
    "    d_model = 128\n",
    "    nhead = 4\n",
    "    num_layers = 2\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel(input_dim, output_dim, d_model, nhead, num_layers).to(device)\n",
    "\n",
    "    # Prepare validation data\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df_hour, input_cols, output_cols, window_size)\n",
    "\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train and validate model\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Predict on new data\n",
    "    new_data = df_15min[input_cols].iloc[-window_size:].values\n",
    "    prediction = predict(model, new_data, device)\n",
    "    print(\"Prediction (buy_price, sell_price, return):\", prediction)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b08cd5-8ce3-4db1-9639-c15a08fe25f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
